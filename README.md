# TCN Pretraining for Multi-Stream VIX Prediction

Standalone TCN pretraining module for learning stream-specific representations from stocks, options, and index data. Pretrained encoders transfer to the main Mamba architecture.

## Overview

Train separate TCN-12 encoders per data stream (stocks, options, index) on SPY 30-day forward Realized Volatility prediction. Each stream learns specialized temporal patterns before integration into the full Mamba model.

## Quick Start

```bash
# Install dependencies
pip install -r requirementstcn.txt

# Precompute top stocks (one-time, for 16GB GPU filtering)
python scripts/compute_top_stocks.py

# Train stocks TCN on RTX 5080
python pretrain_tcn_rv.py --profile rtx5080 --stream stocks

# Train options TCN on H100
python pretrain_tcn_rv.py --profile h100 --stream options

# Train index TCN
python pretrain_tcn_rv.py --profile h100 --stream index
```

## Architecture

Per `architecture.md` specification:
- **3 separate TCN-12 encoders** (not shared weights)
- Each stream → 128-dim embedding
- Auxiliary task: SPY 30d forward RV prediction
- Transfer encoders to: `e_stk`, `e_opt`, `e_idx` in Mamba Level 0

## Data Streams

| Stream | Volume | Filtering | Batch Size (16GB) | Batch Size (80GB) |
|--------|--------|-----------|-------------------|-------------------|
| **Stocks** | ~8M ticks/day (filtered) | Top 100 by volume | 2,000 chunks | 11,600 chunks |
| **Options** | ~5.4M ticks/day | None | 2,800 chunks | 16,000 chunks |
| **Index** | ~347K ticks/day | None | 4,000 chunks | 24,000 chunks |

## GPU Profiles

- **rtx5080**: 16GB VRAM, stock filtering enabled
- **h100/a100**: 80GB VRAM, full universe

## Configuration

Edit `config_pretrain.py`:
- `STREAM_CONFIGS`: Per-stream paths, batch sizing, filtering
- `GPU_PROFILES`: VRAM-aware batch limits
- `PretrainConfig`: TCN architecture, training hyperparameters

## Outputs

Checkpoints saved per stream:
- `checkpoints/tcn_stocks_best.pt` - Full model
- `checkpoints/tcn_stocks_encoder.pt` - Encoder only (for transfer)
- `checkpoints/tcn_options_best.pt`
- `checkpoints/tcn_options_encoder.pt`
- `checkpoints/tcn_index_best.pt`
- `checkpoints/tcn_index_encoder.pt`

## Data Requirements

Expected directory structure:
```
datasets/2022-2023/
├── polygon_stock_trades/     # 461 parquet files
├── options_trades/           # 464 parquet files
├── index_data/               # 468 parquet files (year subdirs)
├── spy_daily_rv.parquet      # Precomputed RV targets
└── top_100_stocks.txt        # Generated by compute_top_stocks.py
```

## Validation

```bash
# Smoke test configuration
python scripts/validate_stream_config.py
```

## Features

- **Background prefetching**: Async file loading while GPU trains
- **On-the-fly filtering**: O(1) ticker lookup, no training stalls
- **Chunk-level batching**: Strict VRAM control
- **Stream-tuned batch sizing**: Optimized per data volume
- **Sample boundary tracking**: Proper loss computation across date boundaries

## Transfer to Mamba

Load pretrained encoders:
```python
# Load stocks encoder
ckpt = torch.load('checkpoints/tcn_stocks_encoder.pt')
frame_encoder.load_state_dict(ckpt['frame_encoder_state_dict'])
chunked_encoder.load_state_dict(ckpt['chunked_encoder_state_dict'])
```

Each stream's encoder slots into the corresponding position in Mamba Level 0.
